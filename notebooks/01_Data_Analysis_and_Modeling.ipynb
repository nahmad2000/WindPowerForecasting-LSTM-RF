{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Power Forecasting: LSTM Direct Approach\n",
    "\n",
    "This notebook performs time-series forecasting of wind turbine power generation using an LSTM model.\n",
    "It follows these steps:\n",
    "1. Load Configuration\n",
    "2. Load Raw Data\n",
    "3. Preprocess Data (resampling, scaling)\n",
    "4. Exploratory Data Analysis (EDA)\n",
    "5. Prepare Data Sequences for LSTM\n",
    "6. Build and Train LSTM Model\n",
    "7. Evaluate Model Performance\n",
    "8. Visualize Results\n",
    "\n",
    "*(Note: Approach 2 (Indirect RF+LSTM) would be added in subsequent sections if implemented)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Markdown # For displaying styled dataframes\n",
    "\n",
    "# --- Add src directory to Python path ---\n",
    "# This allows importing modules from src when the notebook is in notebooks/\n",
    "# Assumes the notebook is run from the project root or the 'notebooks' directory\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    project_root = os.path.abspath(os.path.join('..')) # Go up one level\n",
    "else:\n",
    "    project_root = os.path.abspath(os.path.join('.')) # Assume running from root\n",
    "\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    print(f\"Added '{src_path}' to sys.path\")\n",
    "\n",
    "# Change working directory to project root for consistent file paths\n",
    "if os.getcwd() != project_root:\n",
    "   os.chdir(project_root)\n",
    "   print(f\"Changed working directory to: {project_root}\")\n",
    "\n",
    "# --- Import refactored code modules ---\n",
    "try:\n",
    "    from src import config\n",
    "    from src import data_preprocessing as dp\n",
    "    from src import modeling as mdl\n",
    "    from src import evaluation as evl\n",
    "    from src import plotting as pl\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(\"Ensure you have run 'pip install -r requirements.txt' and the src directory is correct.\")\n",
    "    # Stop execution if imports fail\n",
    "    raise\n",
    "\n",
    "# Apply plot style\n",
    "pl.setup_plot_style()\n",
    "\n",
    "# Ensure results directories exist\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(config.MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(config.IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded. Target variable: {config.TARGET_COL}\")\n",
    "print(f\"Sequence length: {config.SEQUENCE_LENGTH}\")\n",
    "print(f\"Using raw data from: {config.RAW_DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function from data_preprocessing module\n",
    "try:\n",
    "    raw_df = dp.load_data(config.RAW_DATA_FILE)\n",
    "    print(\"\\nRaw data sample:\")\n",
    "    display(raw_df.head())\n",
    "    print(f\"\\nRaw data info:\")\n",
    "    raw_df.info()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    print(f\"\\nPlease ensure the file '{config.RAW_DATA_FILE}' exists.\")\n",
    "    # Stop execution or handle appropriately\n",
    "    raise SystemExit(\"Raw data file not found.\") from e\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform preprocessing steps using the dedicated function\n",
    "try:\n",
    "    processed_df = dp.preprocess_data(\n",
    "        raw_df, \n",
    "        config.DATE_COL, \n",
    "        config.TARGET_COL, \n",
    "        config.FEATURE_COLS, \n",
    "        config.RESAMPLE_FREQ\n",
    "    )\n",
    "    print(\"\\nProcessed data sample:\")\n",
    "    display(processed_df.head())\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred during preprocessing: {e}\")\n",
    "     raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the target variable time series\n",
    "pl.plot_time_series(\n",
    "    processed_df,\n",
    "    config.TARGET_COL,\n",
    "    title=f'{config.TARGET_COL} Over Time ({config.RESAMPLE_FREQ} Resampled)',\n",
    "    ylabel=config.TARGET_COL,\n",
    "    save_path=os.path.join(config.IMAGE_DIR, 'target_timeseries.png')\n",
    ")\n",
    "\n",
    "# Plot distribution of the target variable\n",
    "pl.plot_feature_distribution(\n",
    "    processed_df,\n",
    "    config.TARGET_COL,\n",
    "    title=f'Distribution of {config.TARGET_COL}',\n",
    "    save_path=os.path.join(config.IMAGE_DIR, 'target_distribution.png')\n",
    ")\n",
    "\n",
    "# Plot distribution of a key feature (e.g., Wind Speed)\n",
    "if 'Wind Speed (m/s)' in processed_df.columns:\n",
    "     pl.plot_feature_distribution(\n",
    "         processed_df,\n",
    "         'Wind Speed (m/s)',\n",
    "         title='Distribution of Wind Speed (m/s)',\n",
    "         save_path=os.path.join(config.IMAGE_DIR, 'windspeed_distribution.png')\n",
    "     )\n",
    "else:\n",
    "    print(\"Skipping wind speed distribution plot - column not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data sequentially\n",
    "try:\n",
    "    train_df, val_df, test_df = dp.split_data_sequential(\n",
    "        processed_df, \n",
    "        config.TRAIN_SPLIT, \n",
    "        config.VALIDATION_SPLIT\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Error splitting data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Scale data\n",
    "train_scaled, val_scaled, test_scaled, scaler = dp.scale_data(\n",
    "    train_df, val_df, test_df\n",
    ")\n",
    "\n",
    "# Get the index of the target column AFTER scaling\n",
    "try:\n",
    "    target_col_index = train_scaled.columns.get_loc(config.TARGET_COL)\n",
    "    print(f\"Index of target column '{config.TARGET_COL}' in scaled data: {target_col_index}\")\n",
    "except KeyError:\n",
    "     print(f\"Error: Target column '{config.TARGET_COL}' not found in scaled data columns: {train_scaled.columns}\")\n",
    "     raise\n",
    "\n",
    "# Create sequences\n",
    "X_train, y_train = dp.create_sequences(train_scaled, config.SEQUENCE_LENGTH, target_col_index)\n",
    "X_val, y_val = dp.create_sequences(val_scaled, config.SEQUENCE_LENGTH, target_col_index)\n",
    "X_test, y_test = dp.create_sequences(test_scaled, config.SEQUENCE_LENGTH, target_col_index)\n",
    "\n",
    "if X_train.shape[0] == 0 or X_val.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "    print(\"Warning: One or more sequence datasets (X_train, X_val, X_test) are empty.\")\n",
    "    print(\"This might be due to insufficient data length for the given sequence length.\")\n",
    "    # Optionally raise an error or stop execution\n",
    "    # raise SystemExit(\"Cannot proceed with empty sequence datasets.\")\n",
    "else:\n",
    "    print(\"Sequence shapes:\")\n",
    "    print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    print(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build and Train LSTM Model (Direct Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have data to train on\n",
    "if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
    "    print(\"Skipping model training due to empty training or validation sequences.\")\n",
    "else:\n",
    "    # Define input shape for LSTM\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2]) # (sequence_length, n_features)\n",
    "    print(f\"Input shape for LSTM: {input_shape}\")\n",
    "    \n",
    "    # Build the model\n",
    "    lstm_model = mdl.build_lstm_model(\n",
    "        input_shape,\n",
    "        config.LSTM_UNITS_L1,\n",
    "        config.LSTM_UNITS_L2,\n",
    "        config.LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = mdl.train_model(\n",
    "        lstm_model,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=config.EPOCHS,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        model_checkpoint_path=config.BEST_LSTM_MODEL_FILE,\n",
    "        early_stopping_patience=config.EARLY_STOPPING_PATIENCE\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    pl.plot_learning_curves(\n",
    "        history,\n",
    "        title='LSTM Model Training History',\n",
    "        save_path=os.path.join(config.IMAGE_DIR, 'lstm_learning_curves.png')\n",
    "     )\n",
    "    \n",
    "    model_ready = True\n",
    "    print(\"Model training finished.\")\n",
    "\n",
    "if 'model_ready' not in locals() or not model_ready:\n",
    "    print(\"\\nModel was not trained. Skipping evaluation and visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate only if the model was trained successfully\n",
    "if 'model_ready' in locals() and model_ready:\n",
    "    # Evaluate on Training Data\n",
    "    print(\"\\nEvaluating on Training Set...\")\n",
    "    y_true_train_inv, y_pred_train_inv, metrics_train = evl.evaluate_model(\n",
    "        trained_model, X_train, y_train, scaler, target_col_index, config.METRICS_TO_CALCULATE\n",
    "    )\n",
    "    \n",
    "    # Evaluate on Validation Data\n",
    "    print(\"\\nEvaluating on Validation Set...\")\n",
    "    y_true_val_inv, y_pred_val_inv, metrics_val = evl.evaluate_model(\n",
    "        trained_model, X_val, y_val, scaler, target_col_index, config.METRICS_TO_CALCULATE\n",
    "    )\n",
    "    \n",
    "    # Evaluate on Test Data\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    y_true_test_inv, y_pred_test_inv, metrics_test = evl.evaluate_model(\n",
    "        trained_model, X_test, y_test, scaler, target_col_index, config.METRICS_TO_CALCULATE\n",
    "    )\n",
    "    \n",
    "    # --- Display Metrics --- \n",
    "    metrics_df = pd.DataFrame([\n",
    "        {'Dataset': 'Train', **metrics_train},\n",
    "        {'Dataset': 'Validation', **metrics_val},\n",
    "        {'Dataset': 'Test', **metrics_test}\n",
    "    ]).set_index('Dataset')\n",
    "    \n",
    "    # Format for better display\n",
    "    metrics_df_display = metrics_df.style.format(\"{:.4f}\")\\\n",
    "                                    .set_caption(\"LSTM Direct Approach Performance Metrics\")\\\n",
    "                                    .set_table_styles([{'selector': 'caption', 'props': [('font-size', '16px'), ('font-weight', 'bold')]}])\n",
    "    \n",
    "    print(\"\\n--- Performance Metrics ---\")\n",
    "    display(metrics_df_display) # Use display() in Jupyter for styled output\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_output_path = config.METRICS_LSTM_DIRECT_FILE\n",
    "    try:\n",
    "        metrics_df.to_csv(metrics_output_path)\n",
    "        print(f\"\\nMetrics saved to {metrics_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metrics to {metrics_output_path}: {e}\")\n",
    "        \n",
    "    evaluation_done = True\n",
    "\n",
    "if 'evaluation_done' not in locals() or not evaluation_done:\n",
    "     print(\"\\nModel evaluation was not performed. Skipping result visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results only if evaluation was done\n",
    "if 'evaluation_done' in locals() and evaluation_done:\n",
    "    # Get the correct index for plotting predictions\n",
    "    # The predictions correspond to the time steps AFTER the initial sequence length\n",
    "    # Ensure the dataframes used for indexing still exist and match the sequences\n",
    "    if len(train_df) >= config.SEQUENCE_LENGTH:\n",
    "        train_pred_index = train_df.index[config.SEQUENCE_LENGTH:]\n",
    "    else: train_pred_index = pd.Index([])\n",
    "        \n",
    "    if len(val_df) >= config.SEQUENCE_LENGTH:\n",
    "        val_pred_index = val_df.index[config.SEQUENCE_LENGTH:]\n",
    "    else: val_pred_index = pd.Index([])\n",
    "        \n",
    "    if len(test_df) >= config.SEQUENCE_LENGTH:\n",
    "         test_pred_index = test_df.index[config.SEQUENCE_LENGTH:]\n",
    "    else: test_pred_index = pd.Index([])\n",
    "\n",
    "    # --- Plot actual vs predicted for Test set ---\n",
    "    print(\"\\nPlotting Actual vs Predicted for Test Set...\")\n",
    "    pl.plot_actual_vs_predicted(\n",
    "        y_true_test_inv,\n",
    "        y_pred_test_inv,\n",
    "        test_pred_index, # Use the adjusted index\n",
    "        title='LSTM: Actual vs Predicted Power (Test Set)',\n",
    "        ylabel=config.TARGET_COL,\n",
    "        save_path=os.path.join(config.IMAGE_DIR, 'lstm_actual_vs_predicted_test.png')\n",
    "    )\n",
    "    \n",
    "    # --- Optional: Plot for validation set ---\n",
    "    print(\"\\nPlotting Actual vs Predicted for Validation Set...\")\n",
    "    pl.plot_actual_vs_predicted(\n",
    "        y_true_val_inv,\n",
    "        y_pred_val_inv,\n",
    "        val_pred_index, # Use the adjusted index\n",
    "        title='LSTM: Actual vs Predicted Power (Validation Set)',\n",
    "        ylabel=config.TARGET_COL,\n",
    "        save_path=os.path.join(config.IMAGE_DIR, 'lstm_actual_vs_predicted_val.png')\n",
    "    )\n",
    "    \n",
    "    # --- Optional: Plot for training set (might be crowded) ---\n",
    "    # print(\"\\nPlotting Actual vs Predicted for Training Set...\")\n",
    "    # pl.plot_actual_vs_predicted(\n",
    "    #     y_true_train_inv,\n",
    "    #     y_pred_train_inv,\n",
    "    #     train_pred_index, # Use the adjusted index\n",
    "    #     title='LSTM: Actual vs Predicted Power (Train Set)',\n",
    "    #     ylabel=config.TARGET_COL,\n",
    "    #     save_path=os.path.join(config.IMAGE_DIR, 'lstm_actual_vs_predicted_train.png')\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "The LSTM model (Direct Approach) shows the performance detailed above. Key findings and potential next steps include:\n",
    "\n",
    "* **(Summarize key performance metrics from the test set)**\n",
    "* **(Comment on the visual fit in the Actual vs Predicted plot)**\n",
    "* Implementing and comparing with Approach 2 (Indirect RF+LSTM).\n",
    "* More extensive hyperparameter tuning (e.g., units, sequence length, learning rate, batch size).\n",
    "* Exploring different model architectures or features (e.g., adding weather forecasts if available)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}